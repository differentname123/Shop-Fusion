我本地部署了一个llm聊天大模型，用户可以在网页上进行聊天，但是我觉得页面太丑，太不高级了，你看看有没有开源好用的聊天页面，给我修改后的完整代码：
model_service.py代码如下:
from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import os

app = Flask(__name__)


class ModelService:
    def __init__(self):
        # 模型配置和加载
        os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'
        os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'

        model_name = "rombodawg/Rombos-LLM-V2.5-Qwen-32b"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            llm_int8_threshold=6.0,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        self.model.eval()
        self.system_message = """
You are Qwen, a highly knowledgeable and helpful assistant created by Alibaba Cloud. Please respond in a professional and detailed manner.
        """

    def generate_response(self, user_input):
        messages = [
            {"role": "system", "content": self.system_message},
            {"role": "user", "content": user_input}
        ]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **model_inputs,
                max_new_tokens=1000,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.9
            )
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response


# 初始化模型服务
model_service = ModelService()


@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    user_input = data.get("message", "")
    if not user_input:
        return jsonify({"error": "Invalid input"}), 400
    try:
        response = model_service.generate_response(user_input)
        print(user_input)
        print(response)
        return jsonify({"response": response})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000)

app.py代码如下:
from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import os

app = Flask(__name__)


class ModelService:
    def __init__(self):
        # 模型配置和加载
        os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'
        os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'

        model_name = "rombodawg/Rombos-LLM-V2.5-Qwen-32b"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            llm_int8_threshold=6.0,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        self.model.eval()
        self.system_message = """
You are Qwen, a highly knowledgeable and helpful assistant created by Alibaba Cloud. Please respond in a professional and detailed manner.
        """

    def generate_response(self, user_input):
        messages = [
            {"role": "system", "content": self.system_message},
            {"role": "user", "content": user_input}
        ]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **model_inputs,
                max_new_tokens=1000,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.9
            )
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response


# 初始化模型服务
model_service = ModelService()


@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    user_input = data.get("message", "")
    if not user_input:
        return jsonify({"error": "Invalid input"}), 400
    try:
        response = model_service.generate_response(user_input)
        print(user_input)
        print(response)
        return jsonify({"response": response})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000)

index.html代码如下:
<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="icon" href="/static/favicon.ico" type="image/x-icon">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>奶龙</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f4f4f4;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }

        .chat-container {
            background-color: #fff;
            width: 90%;
            max-width: 500px;
            height: 90%;
            max-height: 700px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .chat-header {
            background-color: #007BFF;
            color: white;
            padding: 10px;
            text-align: center;
        }

        .chat-header h1 {
            font-size: 1.5rem;
            margin: 0;
        }

        .chat-body {
            flex: 1;
            padding: 10px;
            overflow-y: auto;
        }

        .chat-input {
            display: flex;
            border-top: 1px solid #ccc;
        }

        #userInput {
            flex: 1;
            padding: 10px;
            border: none;
            outline: none;
            resize: none;
            height: 50px;
        }

        #sendButton {
            padding: 10px;
            background-color: #007BFF;
            color: white;
            border: none;
            cursor: pointer;
        }

        #sendButton:hover {
            background-color: #0056b3;
        }

        .message {
            margin-bottom: 10px;
        }

        .message.user {
            text-align: right;
        }

        .message.assistant {
            text-align: left;
        }

        .message-content {
            display: inline-block;
            padding: 10px;
            border-radius: 10px;
            max-width: 70%;
        }

        .message.user .message-content {
            background-color: #007BFF;
            color: white;
        }

        .message.assistant .message-content {
            background-color: #f1f1f1;
        }

        .loading-spinner {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #3498db;
            border-radius: 50%;
            width: 20px;
            height: 20px;
            animation: spin 1s linear infinite;
            margin: 0 auto;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        @media (max-width: 600px) {
            .chat-container {
                width: 100%;
                height: 100%;
                max-width: none;
                max-height: none;
            }
        }
    </style>
</head>
<body>
    <div class="chat-container">
        <div class="chat-header">
            <h1>奶龙</h1>
        </div>
        <div class="chat-body" id="responseArea">
        </div>
        <div class="chat-input">
            <textarea id="userInput" placeholder="Type your message"></textarea>
            <button id="sendButton">Send</button>
        </div>
    </div>

    <script>
        const responseArea = document.getElementById('responseArea');
        const userInput = document.getElementById('userInput');
        const sendButton = document.getElementById('sendButton');

        userInput.addEventListener('keydown', (event) => {
            if (event.key === 'Enter' && !event.shiftKey) {
                event.preventDefault();
                sendMessage();
            }
        });

        sendButton.addEventListener('click', sendMessage);

        async function sendMessage() {
            const userMessage = userInput.value.trim();
            if (!userMessage) {
                alert("Please enter a message");
                return;
            }

            const userMessageElement = document.createElement('div');
            userMessageElement.classList.add('message', 'user');
            userMessageElement.innerHTML = `<div class="message-content">${userMessage.replace(/\n/g, '<br>')}</div>`;
            responseArea.appendChild(userMessageElement);
            responseArea.scrollTop = responseArea.scrollHeight;

            userInput.value = '';

            const loadingSpinner = document.createElement('div');
            loadingSpinner.classList.add('loading-spinner');
            responseArea.appendChild(loadingSpinner);
            responseArea.scrollTop = responseArea.scrollHeight;

            try {
                const response = await fetch('https://a5ef-171-212-82-54.ngrok-free.app/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ message: userMessage })
                });
                const data = await response.json();

                loadingSpinner.remove();

                const assistantMessage = document.createElement('div');
                assistantMessage.classList.add('message', 'assistant');
                assistantMessage.innerHTML = `<div class="message-content">${formatResponse(data.response || data.error)}</div>`;
                responseArea.appendChild(assistantMessage);
                responseArea.scrollTop = responseArea.scrollHeight;
            } catch (error) {
                console.error('Error:', error);

                loadingSpinner.remove();

                const errorMessage = document.createElement('div');
                errorMessage.classList.add('message', 'assistant');
                errorMessage.innerHTML = `<div class="message-content">Error: Failed to fetch response</div>`;
                responseArea.appendChild(errorMessage);
                responseArea.scrollTop = responseArea.scrollHeight;
            }
        }

        function formatResponse(response) {
            return response.replace(/\n/g, '<br>');
        }
    </script>
</body>
</html>