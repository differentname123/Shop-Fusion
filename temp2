你看下面是我本地部署的一个模型，现在对它的设置只是一个知识渊博、乐于助人助手，我现在希望把它变成一个擅长商品分析选择的助手，下面是商品选择的要求，你直接给我相应的system_message就行：
1.我会提供多行文字，每一行都是商品描述，包含价格。
2.我希望你能够先按照品牌和规格进行分类，先把相同品牌和规格的商品放在一起，如果规格是重量或者体积等非型号类的商品，只用按照品牌进行分类。
3.然后对于每一类，计算出相应的性价比，即购买同样单位所花费的钱。
4.最后输出每一类最好的商品，即性价比最高的商品，还需要计算平均性价比，即该类所有商品的性价比的平均值，还有最好商品相对于平均性价比的折扣。
import time

from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import os

app = Flask(__name__)


class ModelService:
    def __init__(self):
        # 模型配置和加载
        os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'
        os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'

        model_name = "rombodawg/Rombos-LLM-V2.5-Qwen-32b"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            llm_int8_threshold=6.0,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        self.model.eval()
        self.system_message = """
        你是奶龙，一个由zxh部署的知识渊博、乐于助人助手。请以专业和详细的方式回复。
        """

    def generate_response(self, user_input):
        messages = [
            {"role": "system", "content": self.system_message},
            {"role": "user", "content": user_input}
        ]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **model_inputs,
                max_new_tokens=1000,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.9
            )
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response


# 初始化模型服务
model_service = ModelService()


@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    user_input = data.get("message", "")
    if not user_input:
        return jsonify({"error": "Invalid input"}), 400
    try:
        start_time = time.time()
        response = model_service.generate_response(user_input)
        print(user_input)
        print(response)
        print(f"Time elapsed: {time.time() - start_time}")
        return jsonify({"response": response})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000)